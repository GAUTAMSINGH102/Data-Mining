--------------------------------------------
itemList = []

for i in df:
  itemList.append(df[i])

min_support = 2

def firstFrequentItemSet(itemList, min_support):
  first_frequent_dict = {}
  before_first_frequent_list = []
  first_frequent_list = []
  
  for items in itemList:
    for item in items:
      if item not in first_frequent_dict:
        first_frequent_dict[item] = 1
      else:
        first_frequent_dict[item] += 1

  for item in first_frequent_dict:
    before_first_frequent_list.append(([item], first_frequent_dict[item]))
    if first_frequent_dict[item] >= min_support:
      first_frequent_list.append(([item], first_frequent_dict[item]))

  return first_frequent_list, before_first_frequent_list

firstFrequentList, beforeFirstFrequentList = firstFrequentItemSet(itemList, min_support)

allFrequentItems = []
allFrequentItemsWithSupport = []

for item in firstFrequentList:
  allFrequentItems.append(item[0])
  allFrequentItemsWithSupport.append(item)

def secondFrequentItemSet(firstFrequentList, itemList, min_support):
  before_second_frequent_list = []
  second_frequent_list = []

  firstFreq = []

  for item in firstFrequentList:
    firstFreq.append(item[0][0])

  # print(firstFreq)

  for i in range(len(firstFreq)):
    for j in range(i+1, len(firstFreq)):
      item_1 = firstFreq[i]
      item_2 = firstFreq[j]
      pair = [item_1, item_2]
      # print(pair)

      pair_count = 0

      for items in itemList:
        if item_1 in items and item_2 in items:
          pair_count += 1



      if(pair_count >= min_support):
        listi = [item_1, item_2]
        listi.sort()
        before_second_frequent_list.append((listi, pair_count))
        second_frequent_list.append((listi, pair_count))
        # print(second_frequent_list)
      else:
        listi = [item_1, item_2]
        before_second_frequent_list.append((listi, pair_count))

  return second_frequent_list, before_second_frequent_list

secondFrequentList, beforeSecondFrequentList = secondFrequentItemSet(firstFrequentList, itemList, min_support)

for item in secondFrequentList:
  allFrequentItems.append(item[0])
  allFrequentItemsWithSupport.append(item)

unique = []

for item in firstFrequentList:
  unique.append(item[0][0])

def is_apriori(item_sets, union):
    for item in union:
        subset = union.copy()
        subset.remove(item)
        # print(subset)
        if subset not in item_sets:
          return False
    return True

from itertools import combinations

def threeOrGreater(unique, itemList, allFrequentItems, min_support):
  for size in range(3, len(unique)+1):

    beforeDontKnowList = []
    dontKnowList = []

    for item_set in combinations(unique, size):
      # print(item_set)

      pair_count = 0

      for items in itemList:
        if set(item_set).issubset(items):
          pair_count += 1

      if(pair_count >= min_support):
        if is_apriori(allFrequentItems, list(item_set)):
          beforeDontKnowList.append((list(item_set), pair_count))
          dontKnowList.append((list(item_set), pair_count))
        else:
          beforeDontKnowList.append((list(item_set), pair_count))

    print(f"{size} Before Frequent Item Set")
    print("---------------------")
    print(beforeDontKnowList)
    print("---------------------")
    print("---------------------")
    print(f"{size} Frequent Item Set")
    print("---------------------")
    print(dontKnowList)
    print("---------------------")
    print("---------------------")

    for item in dontKnowList:
      allFrequentItems.append(item[0])
      allFrequentItemsWithSupport.append(item)
    
    if(len(dontKnowList) == 0):
      break

threeOrGreater(unique, itemList, allFrequentItems, min_support)
----------------------------------------------------------------
----------------------------------------------------------------
def random_centroid_intialize(X, k):
  centroids = np.zeros((k, X.shape[1]))

  for i in range(k):
    oneCentroid = X[np.random.choice(range(X.shape[0]))]
    centroids[i] = oneCentroid
  return centroids



def create_cluster(X, centroids, k):
  clusters = [[] for q in range(k)]

  for idx, row in enumerate(X):
    closest_centroid = np.argmin(np.sqrt(np.sum((row-centroids)**2, axis=1)))
    clusters[closest_centroid].append(idx)
  return clusters



def new_centroids(clusters, X, k):
  centroids = np.zeros((k, X.shape[1]))

  for idx, cluster in enumerate(clusters):
    new_centroid = np.mean(X[cluster], axis=0)
    centroids[idx] = new_centroid
  return centroids



def predict_cluster(clusters, X):
  y_pred = np.zeros(X.shape[0])

  for idx, cluster in enumerate(clusters):
    for p in cluster:
      y_pred[p] = idx
  return y_pred



def fit(X, k, niter):
  centroids = random_centroid_intialize(X, k)

  count = 0

  for i in range(niter):

    clusters = create_cluster(X, centroids, k)

    prev_centroids = centroids

    centroids = new_centroids(clusters, X, k)

    diff = centroids - prev_centroids

    if not diff.any():
      break
    else:
      print(count)
      count += 1

  y_pred = predict_cluster(clusters, X)

  return y_pred

--------------------------------------------------------------------
--------------------------------------------------------------------
def sigmoid (x):
    return 1/(1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

#Input datasets
inputs = np.array([[0,0],[0,1],[1,0],[1,1]])
expected_output = np.array([[0],[1],[1],[0]])

epochs = 10000
lr = 0.1

inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1

#Random weights and bias initialization
hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))
hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))
output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))
output_bias = np.random.uniform(size=(1,outputLayerNeurons))

print("Initial hidden weights: ",end='')
print(hidden_weights)
print("Initial hidden biases: ",end='')
print(hidden_bias)
print("Initial output weights: ",end='')
print(output_weights)
print("Initial output biases: ",end='')
print(output_bias)

#Training algorithm
for _ in range(epochs):
	#Forward Propagation
	hidden_layer_activation = np.dot(inputs,hidden_weights)
	hidden_layer_activation += hidden_bias
	hidden_layer_output = sigmoid(hidden_layer_activation)

	output_layer_activation = np.dot(hidden_layer_output,output_weights)
	output_layer_activation += output_bias
	predicted_output = sigmoid(output_layer_activation)

	#Backpropagation
	error = expected_output - predicted_output
	d_predicted_output = error * sigmoid_derivative(predicted_output)
	
	error_hidden_layer = d_predicted_output.dot(output_weights.T)
	d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)

	#Updating Weights and Biases
	output_weights += hidden_layer_output.T.dot(d_predicted_output) * lr
	output_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr
	hidden_weights += inputs.T.dot(d_hidden_layer) * lr
	hidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr


print("Final hidden weights: ",end='')
print(hidden_weights)
print("Final hidden bias: ",end='')
print(hidden_bias)
print("Final output weights: ",end='')
print(output_weights)
print("Final output bias: ",end='')
print(output_bias)

print("\nOutput from neural network after 10,000 epochs: ",end='')
print(predicted_output)z